/-
Copyright (c) 2025 LeanBound Contributors. All rights reserved.
Released under AGPL-3.0 license as described in the file LICENSE.
Authors: LeanBound Contributors
-/
import LeanBound.ML.Network
import LeanBound.Core.IntervalDyadic
import LeanBound.Numerics.IntervalVector

/-!
# Sine Approximator: Trained Neural Network Verification

This file demonstrates **end-to-end verified ML**:
1. A neural network was trained in Python to approximate sin(x)
2. Weights were exported as exact rational numbers
3. We formally verify output bounds using interval arithmetic

## Network Architecture
- Input: x ∈ [0, π]
- Hidden: 4 neurons with ReLU
- Output: approximation of sin(x)

## Verified Property
For all x ∈ [0, π], the network output is bounded.
-/

namespace LeanBound.Examples.ML.SineApprox

open LeanBound.Core
open LeanBound.ML
open LeanBound.Numerics.IntervalVector

/-- Layer 1: 1 → 4 -/
def layer1Weights : List (List ℚ) := [
  [((-1500) : ℚ) / 977],
  [((-1364) : ℚ) / 595],
  [((-1824) : ℚ) / 995],
  [((-867) : ℚ) / 407]
]

def layer1Bias : List ℚ := [
  0, ((-727) : ℚ) / 420, ((-621) : ℚ) / 494, 0
]

def layer1 : Layer where
  weights := layer1Weights
  bias := layer1Bias

/-- Layer 2: 4 → 1 -/
def layer2Weights : List (List ℚ) := [
  [((-403) : ℚ) / 985, ((-1427) : ℚ) / 585, (169 : ℚ) / 980, ((-286) : ℚ) / 943]
]

def layer2Bias : List ℚ := [
  (409 : ℚ) / 649
]

def layer2 : Layer where
  weights := layer2Weights
  bias := layer2Bias

/-- The trained sine approximator (1 → 4 → 1) -/
def sineNet : TwoLayerNet where
  layer1 := layer1
  layer2 := layer2

/-! ## Verification -/

/-- Input domain: [0, π] represented as dyadic interval -/
def inputDomain : IntervalDyadic :=
  IntervalDyadic.ofIntervalRat ⟨0, 22/7, by norm_num⟩ (-53)  -- π ≈ 22/7

/-- Compute output bounds for the entire input domain -/
def outputBounds : IntervalVector :=
  TwoLayerNet.forwardInterval sineNet [inputDomain] (-53)

/-- The computed output bounds (evaluate to see concrete values) -/
#eval outputBounds.map (fun I => (I.lo.toRat, I.hi.toRat))

/-! ## Well-formedness Proofs -/

theorem layer1_wf : layer1.WellFormed := by
  constructor
  · intro row hrow
    simp only [layer1, layer1Weights, Layer.inputDim] at *
    fin_cases hrow <;> rfl
  · rfl

theorem layer2_wf : layer2.WellFormed := by
  constructor
  · intro row hrow
    simp only [layer2, layer2Weights, Layer.inputDim] at *
    fin_cases hrow <;> rfl
  · rfl

theorem sineNet_wf : sineNet.WellFormed := by
  constructor
  · exact layer1_wf
  constructor
  · exact layer2_wf
  · simp [sineNet, layer1, layer2, layer1Weights, layer2Weights,
         Layer.inputDim, Layer.outputDim, layer1Bias]

/-! ## Main Verification Theorem -/

/-- For any x in [0, π], the network output is contained in the computed bounds.

    This is a **formal verification certificate** proving that the trained
    neural network's output is bounded for ALL inputs in the domain. -/
theorem output_bounded {x : ℝ} (hx : 0 ≤ x ∧ x ≤ 22/7) :
    ∀ i, (hi : i < min sineNet.layer2.outputDim sineNet.layer2.bias.length) →
      (TwoLayerNet.forwardReal sineNet [x])[i]'(by
        simp [TwoLayerNet.forwardReal, Layer.forwardReal_length]; omega) ∈
      outputBounds[i]'(by
        simp [outputBounds, TwoLayerNet.forwardInterval, Layer.forwardInterval_length]; omega) := by
  have hwf := sineNet_wf
  have hdim : sineNet.layer1.inputDim = [inputDomain].length := by
    simp [sineNet, layer1, layer1Weights, Layer.inputDim]
  have hxlen : [x].length = [inputDomain].length := rfl
  have hmem : ∀ i, (hi : i < [inputDomain].length) →
      [x][i]'(by omega) ∈ [inputDomain][i]'hi := by
    intro i hi
    simp at hi
    match i with
    | 0 =>
      simp only [List.getElem_cons_zero]
      unfold inputDomain
      apply IntervalDyadic.mem_ofIntervalRat _ (-53) (by norm_num)
      simp only [IntervalRat.mem_def]
      constructor
      · exact_mod_cast hx.1
      · exact_mod_cast hx.2
    | n + 1 => omega
  intro i hi
  have h := TwoLayerNet.mem_forwardInterval hwf hdim hxlen hmem (-53) (by norm_num) i hi
  simp only [outputBounds]
  exact h

/-!
## Interpretation

The theorem `output_bounded` formally proves:

> For ANY real number x ∈ [0, π], the neural network's output
> is guaranteed to lie within the computed interval bounds.

This is verified by the Lean proof assistant - not just tested,
but **mathematically proven** for all infinitely many inputs.

### Applications
- Safety certification for neural network controllers
- Verified bounds on ML model outputs
- Foundation for adversarial robustness proofs
-/

end LeanBound.Examples.ML.SineApprox